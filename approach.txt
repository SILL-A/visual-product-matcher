I built a Visual Product Matcher that finds visually similar fashion products using image embeddings and cosine similarity. The pipeline begins with a curated dataset of ~2.9k catalog images and metadata (product title, category, and image URL/filename). Each product image is encoded into a 512-dimensional embedding using the CLIP ViT-Base model, then saved as a NumPy array for fast retrieval. At query time a user uploads an image (or pastes an image URL); the app computes a CLIP image embedding for that input and measures cosine similarity against the precomputed database embeddings. The top-k nearest items are returned and displayed with product metadata and similarity scores. I implemented the user interface with Streamlit for rapid, Python-only development and easy deployment to Streamlit Cloud. The app includes error handling for broken images, a similarity threshold filter, and caching to minimize repeated model loads. This approach balances accuracy and speed â€” CLIP provides robust visual representations while precomputing embeddings keeps runtime responsive for interactive usage.
